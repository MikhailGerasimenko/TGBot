# ğŸ³ Docker Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ñ‚Ğ°

ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ² Docker ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ°Ñ….

## ğŸ“‹ Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ

- [ĞĞ±Ğ·Ğ¾Ñ€](#Ğ¾Ğ±Ğ·Ğ¾Ñ€)
- [Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ](#Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ)
- [Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚](#Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹-ÑÑ‚Ğ°Ñ€Ñ‚)
- [ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ](#ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ)
- [Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ](#ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ)
- [ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³](#Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ ĞĞ±Ğ·Ğ¾Ñ€

Docker Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ:

- **Model Service** - LLM ÑĞµÑ€Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ GPU
- **Telegram Bot** - Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ñ‚
- **Nginx** - Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
- **Redis** - ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞµÑÑĞ¸Ğ¸
- **Grafana** - Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ (GPU Ğ²ĞµÑ€ÑĞ¸Ñ)

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Docker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx         â”‚    â”‚   Grafana       â”‚    â”‚   Redis         â”‚
â”‚   (Port 80/443) â”‚    â”‚   (Port 3000)   â”‚    â”‚   (Port 6379)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Model Service â”‚    â”‚   Telegram Bot  â”‚    â”‚   Volumes       â”‚
â”‚   (Port 8000)   â”‚â—„â”€â”€â–ºâ”‚   (Internal)    â”‚â—„â”€â”€â–ºâ”‚   (Data/Models) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

- **Docker** 20.10+
- **Docker Compose** 2.0+
- **NVIDIA Container Toolkit** (Ğ´Ğ»Ñ GPU Ğ²ĞµÑ€ÑĞ¸Ğ¸)
- **4GB+ RAM** (8GB+ Ğ´Ğ»Ñ GPU)
- **50GB+ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ°**

### Ğ”Ğ»Ñ GPU Ğ²ĞµÑ€ÑĞ¸Ğ¸

```bash
# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

## ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

### 1. ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°

```bash
# ĞšĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ
git clone <repository-url>
cd corporate-bot

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
cp env.example .env
nano .env  # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ API_TOKEN Ğ¸ ADMIN_CHAT_ID
```

### 2. Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```bash
# Ğ”Ğ»Ñ CPU Ğ²ĞµÑ€ÑĞ¸Ğ¸
./download_model.sh gigachat_20b_q8_0

# Ğ”Ğ»Ñ GPU Ğ²ĞµÑ€ÑĞ¸Ğ¸ (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)
./download_model.sh gigachat_20b_bf16
```

### 3. Ğ—Ğ°Ğ¿ÑƒÑĞº

```bash
# CPU Ğ²ĞµÑ€ÑĞ¸Ñ
cd docker/scripts
chmod +x deploy.sh manage.sh
./deploy.sh cpu

# GPU Ğ²ĞµÑ€ÑĞ¸Ñ
./deploy.sh gpu
```

### 4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°

```bash
# Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
./manage.sh status

# Ğ›Ğ¾Ğ³Ğ¸
./manage.sh logs
```

## âš™ï¸ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

### ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» `.env` Ğ² ĞºĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸:

```bash
# Telegram Bot
API_TOKEN=your_telegram_bot_token
ADMIN_CHAT_ID=your_admin_telegram_id

# Model Service
GGUF_MODEL_PATH=models/model-gigachat_20b_bf16.gguf
LLAMA_CTX=8192
LLAMA_THREADS=16
LLAMA_BATCH=1024
LLAMA_GPU_LAYERS=80
MAX_NEW_TOKENS=512

# RAG System
EMBEDDING_MODEL_NAME=paraphrase-multilingual-MiniLM-L12-v2
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-12-v2

# Database
DATABASE_PATH=data/employees.db

# GPU Optimization
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
```

### ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

#### CPU Ğ²ĞµÑ€ÑĞ¸Ñ
```yaml
# docker-compose.yml
environment:
  - LLAMA_GPU_LAYERS=0
  - LLAMA_CTX=4096
  - LLAMA_THREADS=8
  - LLAMA_BATCH=512
```

#### GPU Ğ²ĞµÑ€ÑĞ¸Ñ
```yaml
# docker-compose.gpu.yml
environment:
  - LLAMA_GPU_LAYERS=80
  - LLAMA_CTX=8192
  - LLAMA_THREADS=16
  - LLAMA_BATCH=1024
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

## ğŸ® Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ

### ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹

```bash
# Ğ—Ğ°Ğ¿ÑƒÑĞº
./manage.sh start

# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°
./manage.sh stop

# ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº
./manage.sh restart

# Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ
./manage.sh status

# Ğ›Ğ¾Ğ³Ğ¸
./manage.sh logs
./manage.sh logs model-service
./manage.sh logs telegram-bot

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
./manage.sh update

# Ğ ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
./manage.sh backup

# Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
./manage.sh restore backup_file.tar.gz
```

### ĞŸÑ€ÑĞ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Docker

```bash
# ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ²
docker ps

# Ğ›Ğ¾Ğ³Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
docker logs corporate-bot-model -f

# Ğ’Ñ…Ğ¾Ğ´ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€
docker exec -it corporate-bot-model bash

# ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²Ğ¸ÑĞ°
docker restart corporate-bot-model

# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²
docker stats
```

## ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

### Health Checks

```bash
# Model Service
curl http://localhost:8000/health

# Nginx
curl http://localhost:80

# Redis
docker exec corporate-bot-redis redis-cli ping
```

### Grafana (GPU Ğ²ĞµÑ€ÑĞ¸Ñ)

- **URL:** http://localhost:3000
- **Ğ›Ğ¾Ğ³Ğ¸Ğ½:** admin
- **ĞŸĞ°Ñ€Ğ¾Ğ»ÑŒ:** admin

### ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸

```bash
# GPU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
nvidia-smi

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
docker stats

# Ğ›Ğ¾Ğ³Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
./manage.sh logs -f
```

## ğŸ”§ Troubleshooting

### Ğ§Ğ°ÑÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹

#### 1. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ÑÑ

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° GPU
nvidia-smi

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
ls -la models/

# Ğ›Ğ¾Ğ³Ğ¸ model service
./manage.sh logs model-service

# ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº
./manage.sh restart
```

#### 2. ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº GPU
docker exec corporate-bot-model nvidia-smi

# ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² .env
LLAMA_GPU_LAYERS=80
LLAMA_CTX=8192
LLAMA_THREADS=16
```

#### 3. ĞÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ²
netstat -tlnp | grep :8000
netstat -tlnp | grep :80

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ²
docker ps

# ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ‚Ğ¸
docker network prune
./manage.sh restart
```

#### 4. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ

```bash
# ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Docker
docker system prune -a

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºĞ°
df -h

# ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ»Ğ¾Ğ³Ğ¾Ğ²
docker system prune -f
```

### ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ°

```bash
# Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ debug Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°
export LOG_LEVEL=DEBUG
./manage.sh restart

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
docker exec corporate-bot-model python -c "from config import *; print('Config OK')"

# Ğ¢ĞµÑÑ‚ API
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "test", "max_tokens": 10}'
```

### Ğ›Ğ¾Ğ³Ğ¸

```bash
# Ğ’ÑĞµ Ğ»Ğ¾Ğ³Ğ¸
./manage.sh logs

# Ğ›Ğ¾Ğ³Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
./manage.sh logs model-service

# Ğ›Ğ¾Ğ³Ğ¸ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸
docker logs corporate-bot-model --timestamps

# Ğ›Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… 100 ÑÑ‚Ñ€Ğ¾Ğº
docker logs corporate-bot-model --tail 100
```

## ğŸ“ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²

```
docker/
â”œâ”€â”€ Dockerfile.bot              # ĞĞ±Ñ€Ğ°Ğ· Telegram Ğ±Ğ¾Ñ‚Ğ°
â”œâ”€â”€ Dockerfile.model            # ĞĞ±Ñ€Ğ°Ğ· LLM ÑĞµÑ€Ğ²Ğ¸ÑĞ°
â”œâ”€â”€ docker-compose.yml          # CPU Ğ²ĞµÑ€ÑĞ¸Ñ
â”œâ”€â”€ docker-compose.gpu.yml      # GPU Ğ²ĞµÑ€ÑĞ¸Ñ
â”œâ”€â”€ nginx.conf                  # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Nginx
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh               # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´ĞµĞ¿Ğ»Ğ¾Ñ
â”‚   â””â”€â”€ manage.sh               # Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸
â””â”€â”€ README.md                   # Ğ­Ñ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ
```

## ğŸ”„ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ

### ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°

```bash
# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
./manage.sh stop

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· git
git pull origin main

# ĞŸĞµÑ€ĞµÑĞ±Ğ¾Ñ€ĞºĞ° Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞº
./manage.sh update
```

### ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```bash
# Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
./download_model.sh gigachat_20b_bf16

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ .env
nano .env  # Ğ˜Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚Ğµ GGUF_MODEL_PATH

# ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº
./manage.sh restart
```

## ğŸ“ ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°

### ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹

```bash
# ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°
./manage.sh status
nvidia-smi
docker stats
df -h

# ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
docker system prune -a
docker volume prune
```

### ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹

- **Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº:** [Ğ’Ğ°ÑˆĞµ Ğ¸Ğ¼Ñ]
- **Email:** [email]
- **Telegram:** [username]

---

**Ğ’ĞµÑ€ÑĞ¸Ñ Docker:** 1.0.0  
**ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ:** 2025-01-19 